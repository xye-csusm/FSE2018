\section{Related Work}
\label{sec:related word}

Related work on word embedding in NLP was discussed in Section~\ref{sec:background}. In this section we discuss other methods for computing word similarities in software engineering and related approaches for bridging the lexical gap in software engineering tasks.

\subsection{Word Similarities in SE}

To the best of our knowledge, word embedding techniques have not been applied before to solve information retrieval tasks in SE. However, researchers \cite{Howard:2013:AMS:2487085.2487155, Wang:2012:ISR:2473496.2473617, 6224276} have proposed methods to infer semantically related software terms, and have built software-specific word similarity databases \cite{6747213, Tian:2014:SSW:2591062.2591071}.

Tian et al. \cite{6747213, Tian:2014:SSW:2591062.2591071} introduce a software-specific word similarity database called SEWordSim that was trained on StackOverflow questions and answers. They represent words in a high-dimensional space in which every element within the vector representation of word $w_i$ is the Positive Pointwise Mutual Information (PPMI) between $w_i$ and another word $w_j$ in the vocabulary. Because the vector space dimension equals the vocabulary size, the scalability of their vector representation is limited by the size of the vocabulary. When the size of the training corpus grows, the growing vector dimension will lead to both larger time and space complexities. Recent studies \cite{baroni-etal-2014b, Mikolov:ICLR} also showed that this kind of traditional count-based language models were outperformed by the neural-network-based low-dimensional word embedding models on a wide range of word similarity tasks.

Howard et al. \cite{Howard:2013:AMS:2487085.2487155} and Yang et al. \cite{6224276} infer semantically related words directly from comment-code, comment-comment, or code-code pairs without creating the distributional vector representations. They first need to map a line of comment (or code) to another line of comment (or code), and then infer word pairs from these line pairs. Similarly, Wang et al. \cite{Wang:2012:ISR:2473496.2473617} infer word similarities from tags in FreeCode. The main drawback of these approaches is that they rely solely on code, comments, and tags. More general free-text contents are ignored. Many semantically related words (e.g. ``placeholder'' and ``view'') are not in the source code but in the free-text contents of project documents (e.g. the Eclipse user guide, developer guide, and API document shown in Figure~\ref{fig:guide1} to Figure~\ref{fig:api}). However, these types of documents are not exploited in these approaches.

More importantly, all the above approaches did not explain how word similarities can be used to estimate document similarities. They reported user studies in which human subjects were recruited to evaluate whether the word similarities are accurate. However, these subjective evaluations do not tell whether and how word similarities can be used in solving IR tasks in SE.

\subsection{Bridging the Lexical Gap to Support \\Software Engineering Tasks}

Text retrieval techniques have been shown to help in various SE tasks \cite{Haiduc:2013:AQR:2486788.2486898, marcus2012use}. However, the system performance is usually suboptimal due to the lexical gap between user queries and code \cite{5989838}. To bridge the lexical gap, a number of approaches \cite{Bajracharya:2010:LUS:1882291.1882316, Chatterjee:2009:SSE:1533013.1533048, Dasgupta:2013:EST:2550526.2550586, 5989838, Stylos:2006:MWT:1174509.1174678, Ye:FSE14} have been recently proposed that exploit information from API documentations. These approaches extract API entities referenced in code, and use the corresponding documentations to enhance the ranking results.

Specifically, McMillan et al. \cite{5989838} measure the lexical similarity between the user query and API entities, then rank higher the code that uses the API entities with higher similarity scores. Bajracharya et al. \cite{Bajracharya:2010:LUS:1882291.1882316} augment the code with tokens from other code segments that use the same API entries. Ye et al. \cite{Ye:FSE14} concatenate the descriptions of all API entries used in the code, and directly measure the lexical similarity between the query and the concatenated document. The main drawback of these approaches is that they consider only the API entities used in the code. The documentations of other API entities are not used. Figure~\ref{fig:br1} shows the Eclipse bug 384108. Figure~\ref{fig:file} shows its relevant file {\it PartServiceImpl.java}. Figure~\ref{fig:api} shows the description of an API entry {\it IPageLayout}. Although {\it IPageLayout} is not used in {\it PartServiceImpl.java}, its API descriptions contains useful information that can help bridge the lexical gap by mapping the term ``view'' in bug 384108 with the term ``placeholder'' in {\it PartServiceImpl.java}. Therefore, to bridge the lexical gap, we should consider not only the descriptions of the API entities used in the code but also all API documents and project documents (e.g. the user guide shown in Figure~\ref{fig:guide1} and the developer guide in Figure~\ref{fig:guide2}) that are available.

Latent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) have been used in the area of feature location and bug localization. Poshyvanyk et al. \cite{Poshyvanyk:2007:FLU:1263152.1263534, Poshyvanyk:2006:CPR:1135772.1136177} use LSI to reduce the dimension of the term-document matrix, represent code and queries as vectors, and estimate the similarity between code and queries using the cosine similarity between their vector representations. Similarly, Nguyen et al. \cite{Nguyen:2011:TAN:2190078.2190181} and Lukins et al. \cite{Lukins:2010:BLU:1824820.1824850} use LDA to represent code and queries as topic distribution vectors. Rao et al. \cite{Rao:2011:RSL:1985441.1985451} compare various IR techniques on bug localization, and report that traditional IR techniques such as VSM and Unigram Model (UM) outperform the more sophisticated LSI and LDA techniques. These approaches create vector representations for documents instead of words and estimate query-code similarity based on the cosine similarity between their vectors. McMillan et al. \cite{Poshyvanyk:2013:CLU:2377656.2377660} introduced a LSI-based approach for measuring program similarity, and showed that their model achieve higher precision than a LSA-based approach in detecting similar applications. All these works neither measure word similarities nor try to bridge the lexical gap between code and queries.
