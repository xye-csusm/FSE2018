\section{Evaluation}
\label{sec:evaluation}

In this section, we describe an extensive set of experiments that are intended to determine the utility of the new document similarity measures based on word embeddings in the context of bug localization. This is an information retrieval task in which queries are bug reports and the system is trained to identify relevant, buggy files.

\begin{table*}[!thbp]
\centering
\caption{Benchmark Projects: {\it Eclipse$^*$ refers to Eclipse Platform UI.}}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline
Project&Time Range&\# of bug reports&\# of bug reports&\# of bug reports&total\\
&&used for testing&used for training&used for tuning&\\ \hline
Birt&2005-06-14 -- 2013-12-19&583&500&1,500&2,583\\ \hline
Eclipse$^*$ &2001-10-10 -- 2014-01-17&1,656&500&1,500&3,656\\ \hline
JDT&2001-10-10 -- 2014-01-14&632&500&1,500&2,632\\ \hline
SWT&2002-02-19 -- 2014-01-17&817&500&1,500&2,817\\ \hline
\end{tabular}
\label{tab:dataset}
\end{table*}

\subsection{Text Pre-processing}
\label{sec:tokenization}

There are three types of text documents used in the experimental evaluations in this section: 1) the Eclipse API reference, developer guides, Java API reference, and Java tutorials that are used to train the word embeddings; 2) the bug reports; and 3) the source code files. When creating bag-of-words for these documents, we use the same pre-processing steps on all of them: we remove punctuation and numerical numbers, then split the text by whitespace. 

The tokenization however is done differently for each category of documents. In the one-vocabulary setting, compound words in the bug reports and the source code files are split based on capital letters. For example, ``WorkbenchWindow'' is split into ``Workbench'' and ``Window'', while its original form is also reserved. We then apply the Porter stemmer on all words/tokens.

In the two-vocabulary setting, a code token such as a method name ``{\it clear} is marked as ``@clear@'' so that it can be distinguished from the adjective ``clear''. Then we stem only the natural language words. We also split compound natural language words. In order to separate code tokens from natural language words in the training corpus, we wrote a dedicated HTML parser to recognize and mark the code tokens. For bug reports, we mark words that are not in an English dictionary as code tokens. For source code files, all tokens except those in the comments are marked as code tokens. Inside the comments, words that are not in an English dictionary are also marked as code tokens. 

\subsection{Corpus for Training Word Embeddings}
\label{sec:evaluation:training corpus}

To train the shared embeddings, we created a corpus from documents in the following Eclipse repositories: the Platform API Reference, the JDT API Reference, the Birt API Reference, the Java SE 7 API Reference, the Java tutorials, the Platform Plug-in Developer Guide, the Workbench User Guide, the Plug-in Development Environment Guide, and the JDT Plug-in Developer Guide. The number of documents and words/tokens in each repository are shown in Table~\ref{tab:corpus}. All documents are downloaded from their official website\footnote{\url{http://docs.oracle.com/javase/7/docs}}\footnote{\url{http://www.eclipse.org/documentation}}.  Code tokens in these documents are usually placed between special HTML tags such as $\langle${\it code}$\rangle$ or emphasized with different fonts.

\begin{table}[thbp]
\centering
\caption{Documents for training word embeddings.}
\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|c|} \hline
Data sources & Documents & Words/Tokens\\ \hline \hline
Platform API Reference & 3,731 & 1,406,768\\ \hline
JDT API Reference & 785 & 390,013\\ \hline
Birt API Reference & 1,428 & 405,910\\ \hline
Java SE 7 API Reference & 4,024 & 2,840,492\\ \hline
The Java Tutorials  & 1,282 & 1,024,358\\ \hline
Platform Plug-in Developer Guide & 343 & 182,831\\ \hline
Workbench User Guide & 426 & 120,734\\ \hline
Plug-in Development Environment Guide & 269 & 90,356\\ \hline
JDT Plug-in Developer Guide & 164 & 64,980\\ \hline
Total & 12,452 & 6,526,442\\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:corpus}
\end{table}
\vspace*{-1em}
\begin{table}[ht!]
\centering
\caption{The vocabulary size.}
\begin{tabular}{|c|c|} \hline
Word embeddings trained on: & Vocabulary size\\ \hline
one-vocabulary setting & 21,848\\ \hline
two-vocabulary setting & 25,676\\ \hline
\end{tabular}
\label{tab:vocabulary}
\end{table}
\vspace*{-1em}
\begin{table}[ht!]
\centering
\caption{Number of word pairs.}
\begin{tabular}{|c|c|} \hline
Approach & \# of word pairs\\ \hline
One-vocabulary embeddings & 238,612,932\\ \hline
Two-vocabulary embeddings & 329,615,650\\ \hline
SEWordSim \cite{Tian:2014:SSW:2591062.2591071} & 5,636,534\\ \hline
SWordNet \cite{6224276} & 1,382,246\\ \hline
\end{tabular}
\label{tab:number of word pairs}
\end{table}

To learn the shared embeddings, we used the Skip-gram model, modified such that it works in the training scenarios described in Section~\ref{sec:embeddings2}. Table~\ref{tab:vocabulary} shows the number of words in each vocabulary setting.  Table~\ref{tab:number of word pairs} compares the number of word pairs used to train word embeddings in the one- and two-vocabulary settings with the number of word pairs used in two related approaches. Thus, when word embeddings are trained on the one-vocabulary setting, the vocabulary size is 21,848, which leads to 238,612,932 word pairs during training. This number is over 40 times the number of word pairs in SEWordSim \cite{Tian:2014:SSW:2591062.2591071}, and is more than 172 times the number of word pairs in SWordNet \cite{6224276}.

\subsection{Benchmark Datasets}
\label{sec:evaluation:subject systems}

We perform evaluations on the fined-grained benchmark dataset from \cite{Ye:FSE14}. Specifically, we use four open-source Java projects: Birt\footnote{\url{https://www.eclipse.org/birt/}}, Eclipse Platform UI\footnote{\url{http://projects.eclipse.org/projects/eclipse.platform.ui}}, JDT\footnote{\url{http://www.eclipse.org/jdt/}}, and SWT\footnote{\url{http://www.eclipse.org/swt/}}. For each of the 10,000 bug reports in this dataset, we {\it checkout} a before-fixed version of the source code, within which we rank all the source code files for the specific bug report.

Since the training corpus for word embeddings (shown in Table~\ref{tab:corpus}) contains only Java SE 7 documents, for testing we use only bug reports that were created for Eclipse versions starting with 3.8, which is when Eclipse started to add Java SE 7 support. The Birt, JDT, and SWT projects are all Eclipse Foundation projects, and also support Java SE 7 after the Eclipse 3.8 release. Overall, we collect for testing 583, 1656, 632, and 817 bug reports from Birt, Eclipse Platform UI, JDT, and SWT, respectively. Older bug reports that were reported for versions before release 3.8 are used for training and tuning the learning-to-rank systems.

Table~\ref{tab:dataset} shows the number of bug reports from each project used in the evaluation. The methodology used to collect the bug reports is discussed at length in \cite{Ye:FSE14}. Here we split the bug reports into a testing set, a training set, and a tuning set. Taking Eclipse Platform UI for example, the newest 1,656 bug reports, which were reported starting with Eclipse 3.8, are used for testing. The older 500 bug reports in the training set are used for learning the weight parameters of the ranking function in Equation~\ref{eq:file score}, using the SVM$^{rank}$ package \cite{Joachims:2002:OSE:775047.775067, Joachims:2006:TLS:1150402.1150429}. The oldest 1,500 bug reports are used for tuning the hyper-parameters of the Skip-gram model and the $SVM^{rank}$ model, by repeatedly training on 500 and testing on 1000 bug reports. To summarize, we tune the hyper-parameters of the Skip-gram model and the SVM$^{rank}$ model on the tuning dataset, then train the weight vector used in the ranking function on the training dataset, and finally test and report the ranking performance on the testing dataset. After tuning, the Skip-gram model was train to learn embeddings of size 100, with a context window of size 10, a minimal word count of 5, and a negative sampling of 25 words. 


\subsection{Results and Analysis}
\label{sec:evaluation:results and analysis}

We ran extensive experiments for the bug localization task, in order to answer the following research questions:
\begin{enumerate}
  \item[\textit{RQ1:}] Do word embeddings help improve the ranking performance, when added to an existing strong baseline?
  \item[\textit{RQ2:}] Do word embeddings trained on different corpora change the ranking performance?
  \item[\textit{RQ3:}] Do the word embedding training heuristics improve the ranking performance, when added to the vanilla Skip-gram model?
  \item[\textit{RQ4:}] Do the modified text similarity functions improve the ranking performance, when compared with the original similarity function in \cite{mihalcea:aaai06}?
\end{enumerate}

We use the Mean Average Precision (MAP) \cite{Manning:2008:IIR:1394399}, which is the mean of the average precision values for all queries, and the Mean Reciprocal Rank (MRR) \cite{Voorhees99thetrec-8}, which is the harmonic mean of ranks of the first relevant documents, as the evaluation metrics. MAP and MRR are standard evaluation metrics in IR, and were used previously in related work on bug localization \cite{Saha:2013:ASE:6693093, Ye:FSE14,Zhou:2012:BFM:2337223.2337226}.


\subsubsection{\textbf{RQ1:} Do word embeddings help improve the ranking performance?}
\label{sec:evaluation:rq1}

\begin{table}[t]
\centering
\caption{MAP and MRR for the 5 ranking systems.}
\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline
Project & Metric & LR+WE$^1$ & LR+WE$^2$ & LR & WE$^1$ & WE$^2$\\
& & $\phi_1$-$\phi_8$ & $\phi_1$-$\phi_8$ & $\phi_1$-$\phi_6$ & $\phi_7$-$\phi_8$ &$\phi_7$-$\phi_8$ \\ \hline
Eclipse & MAP & 0.40 & 0.40 & 0.37 & 0.26 & 0.26\\
Platform UI& MRR & 0.46 & 0.46 & 0.44 & 0.31 & 0.31\\ \hline
JDT& MAP & 0.42 & 0.42 & 0.35 & 0.22 & 0.23\\
& MRR & 0.51 & 0.52 & 0.43 & 0.27 & 0.29\\ \hline
SWT& MAP & 0.38 & 0.38 & 0.36 & 0.25 & 0.25\\
& MRR & 0.45 & 0.45 & 0.43 & 0.30 & 0.30\\ \hline
Birt& MAP & 0.21 & 0.21 & 0.19 & 0.13 & 0.13\\
& MRR & 0.27 & 0.27 & 0.24 & 0.17 & 0.17\\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:comparison}
\end{table}

The results shown in Table~\ref{tab:comparison} compare the \textbf{LR} system introduced in  \cite{Ye:FSE14} with a number of systems that use word embeddings in the one- and two-vocabulary settings, as follows: \textbf{LR+WE$^1$} refers to combining the one-vocabulary word-embedding-based features with the six features of the LR system from \cite{Ye:FSE14}, \textbf{LR+WE$^2$} refers to combining the two-vocabulary word-embedding-based features with the LR system, \textbf{WE$^1$} refers to using only the one-vocabulary word-embedding-based features, and \textbf{WE$^2$} refers to using only the two-vocabulary word-embedding-based features. The parameter vector of each ranking system is learned automatically. The results show that the new word-embedding-based similarity features, when used as additional features, improve the performance of the \textbf{LR} system. The results of both \textbf{LR+WE$^1$} and \textbf{LR+WE$^2$} show that the new features help achieve 8.1\%, 20\%, 5.6\%, and 16.7\% relative improvements in terms of MAP over the original \textbf{LR} approach, for Eclipse Platform UI, JDT, SWT, and Birt respectively. In \cite{Ye:FSE14}, \textbf{LR} was reported to outperform other state-of-the-art bug localization models such as the VSM-based BugLocator from Zhou et al. \cite{Zhou:2012:BFM:2337223.2337226} and the LDA-based BugScout from Nguyen et al. \cite{Nguyen:2011:TAN:2190078.2190181}.

Another observation is that using word embeddings trained on one-vocabulary and using word embeddings trained on two-vocabulary achieve almost the same results.
% The advantage of using two-vocabulary is that natural language words such as the adjective \textit{clear} can be distinguished from the method name {\tt @clear@}, so the Skip-gram model would not use the context of the method name {\tt @clear@} to train the word embedding for the adjective \textit{clear}. But the results do not show a significant difference.
By looking at a sample of API documents and code, we discovered that class names, method names, and variable names are used with a consistent meaning throughout. For example, developers use \textit{Window} to name a class that is used to create a window instance, and use \textit{open} to name a method that performs an open action. Therefore, we believe the two-vocabulary setting will be more useful when word embeddings are trained on both software engineering (SE) and natural language (NL) corpora (e.g. Wikipedia), especially in situations in which a word has NL meanings that do not align well with its SE meanings. For example, since {\it eclipse} is used in NL mostly with the astronomical sense, it makes sense for {\it eclipse} to be semantically more similar with {\it light} than {\it ide}. However, in SE, we want {\it eclipse} to be more similar to {\it ide} and {\it platform} than to {\it total}, {\it color},  or {\it light}. By training separate embeddings for {\it eclipse} in NL contexts (i.e. {\it eclipse\_NL}) vs. {\it eclipse} in SE contexts (i.e. {\it eclipse\_SE}), the expectation is that, in an SE setting, the {\it eclipse\_SE} embedding would be more similar with the {\it ide\_SE} embedding than the {\it total\_SE} or {\it color\_SE} embeddings. 
% This would be especially useful when the word embeddings are trained on both SE text and Wikipedia text.
% Thus, although we distinguish {\tt @Window@} in code from \textit{window} in natural languages, their trained word embeddings are very similar. We believe this is the main reason why \textbf{LR+WE$^1$} and \textbf{LR+WE$^2$} achieve almost the same results. 

Kochhar et al. \cite{Kochhar:2014:PBB:2642937.2642997} reported from an empirical study that the localized bug reports, which explicitly mention the relevant file names, ``{\it statistically significantly and substantially}'' impact the bug localization results. They suggested that there is no need to run automatic bug localization techniques on these bug reports. Therefore, we separate the testing bug reports for each project into two subsets T1 (easy) and T2 (difficult). Bug reports in T1 mention either the relevant file names or their top-level public class names, whereas T2 contains the other bug reports. Note that, although bug reports in T1 make it easy for the programmer to find a relevant buggy file, there may be other relevant files associated with the same bug report that could be more difficult to identify, as shown in the statistics from Table~\ref{tab:comparison on T1 and T2}.

%\begin{table}[ht]
%\centering
%\caption{Number of bug reports that contain (T1) and do not contain (T2) the relevant file names.}
%\begin{tabular}{|c|c|c|c|} \hline
%Project & T1 & T2 & Total for testing \\ \hline
%Eclipse Platform UI& 322 & 1,334 & 1,656\\ \hline
%JDT & 84 & 548 & 632 \\ \hline
%SWT & 376 & 441 & 817 \\ \hline
%Birt & 27 & 556 & 583 \\ \hline
%\end{tabular}
%\label{tab:number of bug reports in T1 and T2}
%\end{table}

\begin{table}
\centering
\caption{Results on easy (T1) vs. difficult (T2) bug reports, together with \# of bug reports (size) and average \# of relevant files per bug report (avg).}
\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|cc|cc|} \cline{3-6}
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{T1} & \multicolumn{2}{c|}{T2} \\ \cline{3-6}
\multicolumn{2}{c|}{} & LR+WE$^1$ & LR & LR+WE$^1$ & LR \Tstrut\Bstrut\\ \hline
 & Size/Avg & \multicolumn{2}{c|}{322/2.11} & \multicolumn{2}{c|}{1,334/2.89} \\ \cline{2-6}
Eclipse & MAP & 0.80 & 0.78 & 0.30 & 0.27 \\
 & MRR & 0.89 & 0.87 & 0.36 & 0.33 \\ \hline
 & Size/Avg & \multicolumn{2}{c|}{84/2.60} & \multicolumn{2}{c|}{548/2.74} \\ \cline{2-6}
JDT & MAP & 0.79 & 0.75 & 0.36 & 0.29 \\
& MRR & 0.90 & 0.87 & 0.45 & 0.37 \\ \hline
 & Size/Avg & \multicolumn{2}{c|}{376/2.35} & \multicolumn{2}{c|}{441/2.57} \\ \cline{2-6} 
SWT & MAP & 0.57 & 0.55 & 0.22 & 0.21 \\
& MRR & 0.66 & 0.65 & 0.27 & 0.26 \\ \hline
 & Size/Avg & \multicolumn{2}{c|}{27/2.48} & \multicolumn{2}{c|}{556/2.24} \\ \cline{2-6}
Birt & MAP & 0.48 & 0.54 & 0.20 & 0.17 \\
& MRR & 0.62 & 0.69 & 0.25 & 0.22 \\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:comparison on T1 and T2}
\end{table}

Table~\ref{tab:comparison on T1 and T2} shows the MAP and MRR results on T1 and T2. Because \textbf{LR+WE$^1$} and \textbf{LR+WE$^2$} are comparable on the test bug reports, here we compare only \textbf{LR+WE$^1$} with \textbf{LR}. The results show that both \textbf{LR+WE$^1$} and \textbf{LR} achieve much better performance on bug reports in T1 than T2 for all projects. This confirms the conclusions of the empirical study from Kochhar et al. \cite{Kochhar:2014:PBB:2642937.2642997}. 
% For bug reports in T1 that already mention the buggy file names, developers may easily locate the bug. However, the ranking system cannot achieve 100\% precision on these localized reports. So just as Kochhar et al. suggested \cite{Kochhar:2014:PBB:2642937.2642997}, we may not need to run automatic localization approaches for these bug reports.
The results in Table~\ref{tab:comparison on T1 and T2} also show that overall using word embeddings helps on both T1 and T2. One exception is Birt, where the use of word embeddings hurts performance on the 27 easy bugs in T1, a result that deserves further analysis in future work.

To summarize, we showed that using word embeddings to create additional semantic similarity features helps improve the ranking performance of a state-of-the-art approach to bug localization. However, separating the code tokens from the natural language words in two vocabularies when training word embeddings on the SE corpus did not improve the performance. In future work, we plan to investigate the utility of the two-vocabulary setting when training with both SE and NL corpora.


\subsubsection{\textbf{RQ2:} Do word embeddings trained on a different corpora change the ranking performance?}
\label{sec:evaluation:rq3}

To test the impact of the training corpus, we train word embeddings in the one-vocabulary setting using the Wiki data dumps\footnote{\url{https://dumps.wikimedia.org/enwiki/}}, and redo the ranking experiment.
% I compare the results of this new experiment with the results shown in Table~\ref{tab:comparison}.
\begin{table}[thbp]
\centering
\caption{The size of the different corpora.}
\begin{tabular}{|c|c|c|} \hline
Corpus & Vocabulary & Words/Tokens\\ \hline \hline
Eclipse and Java & 21,848 & 6,526,442\\ \hline
% documents & & \\ \hline
Wiki & 2,098,556 & 3,581,771,341\\ \hline
\end{tabular}
\label{tab:different corpora}
\end{table}
%\vspace*{-1em}
\begin{table}[thbp]
\centering
\caption{Comparison of the \textbf{LR+WE$^1$} results when using word embeddings trained on different corpora.}
\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|c|c|c|c|} \hline
Corpus & Metric & Eclipse & JDT & SWT & Birt \\
& Metric & Platform UI & & & \\ \hline
Eclipse and Java & MAP & 0.40 & 0.42 & 0.38 & 0.21 \\
documents& MRR & 0.46 & 0.51 & 0.45 & 0.27 \\ \hline
Wiki & MAP & 0.40 & 0.41 & 0.38 & 0.21 \\
& MRR & 0.46 & 0.51 & 0.45 & 0.27 \\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:comparison on different copora}
\end{table}
The advantage of using the Wiki corpus is its large size for training.
Table~\ref{tab:different corpora} shows the size of the Wiki corpus. The number of words/tokens in the Wiki corpus is 548 times the number in our corpus, while its vocabulary size is 96 times the vocabulary size of our corpus. Theoretically, the larger the size of the training corpus the better the word embeddings. On the other hand, the advantage of the smaller training corpus in Table~\ref{tab:corpus} is that its vocabulary is close to the vocabulary used in the queries (bug reports) and the documents (source code files).

Table~\ref{tab:comparison on different copora} shows the ranking performance by using the Wiki embeddings. Results show that the project specific embeddings achieve almost the same MAP and MRR for all projects as the Wiki embeddings. We believe one reason for the good performance of the Wiki embeddings is the pre-processing decision to split compound words such as {\tt WorkbenchWindow} that do not appear in the Wiki vocabulary into their components words {\tt Workbench} and {\tt Window}, which belong to the Wiki vocabulary. Correspondingly, Table~\ref{tab:wiki-vs-eclipse} below shows the results of evaluating just the word-embeddings features ({\bf WE}$^1$) on the Eclipse project with the two types of embeddings, with and without splitting compound words. As expected, the project-specific embeddings have better performance than the Wiki-trained embeddings when compound words are not split; the comparison is reversed when splitting is used.
\begin{table}[h]
\centering
\caption{Project-specific vs. Wikipedia embeddings performance of WE$^1$ features, with and without splitting compound words.}
%\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|c|c|} \hline
Project & Metric & No Split & Split \\ \hline
Eclipse/Java& MAP & 0.254 & 0.260 \\
 & MRR & 0.307 & 0.310 \\ \hline
Wikipedia & MAP & 0.248 & 0.288 \\
& MRR & 0.300 & 0.346 \\ \hline
\end{tabular}
%\end{adjustbox}
\label{tab:wiki-vs-eclipse}
\end{table}
Overall, each corpus has its own advantages: while the embeddings trained on the project-specific corpus may better capture specific SE meanings, the embeddings trained on Wikipedia may benefit from the substantially larger amount of training examples. Given the complementary advantages, in future work we plan to investigate training strategies that exploit both types of corpora.

%\begin{table}[htbp]
%\centering
%\caption{The vocabulary overlap between word embeddings trained on different corpora.}
%\begin{adjustbox}{width=0.47\textwidth}
%\begin{tabular}{|c|c|c|c|c|} \hline
% & Code & Reports & Java and Eclipse & Wiki \\ \hline
% Code & 51,747 & 7,368 & 8,174 & 7,960 \\ \hline
% Reports & 7,368 & 11,167 & 4,492 & 4,374 \\ \hline
% Java and Eclipse & 8,174 & 4,492 & 21,848 & 6,935 \\ \hline
% Wiki & 7,960 & 4,374 & 6,935 & 1,811,253\\ \hline
%\end{tabular}
%\end{adjustbox}
%\label{tab:corpus overlap}
%\end{table}
%Table~\ref{tab:corpus overlap} shows the overlap between the vocabularies of word embeddings trained on different corpora. The vocabulary of the Wiki embeddings in Table~\ref{tab:corpus overlap} is smaller than the vocabulary of the Wiki corpus in Table~\ref{tab:different corpora} because we perform stemming. In Table~\ref{tab:corpus overlap}, ``Code'' contains all the source code files of the Eclipse Platform UI project checkout at {\it commit 5da5952}, which is the before-fix version of the newest bug report in our benchmark dataset, ``Reports'' contains all the Eclipse Platform UI bug reports in our dataset, ``Java and Eclipse documents'' refers to the training corpus shown in Table~\ref{tab:corpus}, and ``Wiki'' refers to the Wiki corpus. As we can see from Table~\ref{tab:corpus overlap}, the vocabulary overlap between ``Code'' and ``Wiki'' is close to the vocabulary overlap between ``Code'' and ``Java and Eclipse Documents''. Similarly, the vocabulary overlap between ``Reports'' and ``Wiki'' is also close to the vocabulary overlap between ``Reports'' and ``Java and Eclipse Documents''. Although the Wiki corpus is huge, the vocabulary of bug reports and the source code files is small. Thus, the large size of the Wiki corpus does not translate into improved performance on the bug localization task. % Therefore, using a small corpus that contains Java and Eclipse project documents can achieve the same results with using the Wiki corpus. 


\subsubsection{\textbf{RQ3:} Do the word embedding training heuristics improve the ranking performance?}
\label{sec:evaluation:rq4}

Table~\ref{tab:comparison of skip-gram} shows the results of using the original Skip-gram model without applying the heuristic techniques discussed in Sections~\ref{sec:mapping1} and~\ref{sec:mapping2}. It shows that both the enhanced and the original Skip-gram model achieve the same results most of the time. These results appear to indicate that increasing the number of training pairs for word embeddings will not lead to further improvements in ranking performance, which is compatible with the results of using the Wiki corpus vs. the much smaller project-specific corpora.
\begin{table}[t]
\centering
\caption{\textbf{LR+WE$^1$} results obtained using the enhanced vs. the original Skip-gram model.}
\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline
Project & Metric & LR & Enhanced Skip-gram & Original Skip-gram\\
& & $\phi_1$-$\phi_8$ & $\phi_1$-$\phi_6$ & $\phi_1$-$\phi_8$ \\ \hline
Eclipse & MAP & 0.37 & 0.40 & 0.40 \\
Platform UI& MRR & 0.44 & 0.46 & 0.46 \\ \hline
JDT& MAP & 0.35 & 0.42 & 0.42 \\
& MRR & 0.43 & 0.51 & 0.51 \\ \hline
SWT& MAP & 0.36 & 0.38 & 0.37 \\
& MRR & 0.43 & 0.45 & 0.44 \\ \hline
Birt& MAP & 0.19 & 0.21 & 0.21 \\
& MRR & 0.24 & 0.27 & 0.27 \\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:comparison of skip-gram}
\end{table}

\subsubsection{\textbf{RQ4:} Do the modified text similarity functions improve the ranking performance?}
\label{sec:evaluation:rq5}

Table~\ref{tab:comparison of text similarity function} below compares the new text similarity functions shown in Equation~\ref{eq:new tssim} with the original text similarity function from Mihalcea et al. \cite{mihalcea:aaai06}, shown in Equation~\ref{eq:tssim}. In \textbf{WE$^1_{ori}$}, the new features $\phi_7$ and $\phi_8$ are calculated using the one-vocabulary word embeddings and the original $idf$-weighted text similarity function. The results of \textbf{LR+WE$^1$} and \textbf{LR} are copied from Table~\ref{tab:comparison}, for which $\phi_7$ and $\phi_8$ are calculated using the new text similarity functions.

\begin{table}[ht]
\centering
\caption{Comparison between the new text similarity function (LR+WE$^1$) and the original similarity function (LR+WE$^1_{ori}$).}
\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline
Project & Metric & LR & LR+WE$^1$ & LR+WE$^1_{ori}$ \\
& & $\phi_1$-$\phi_8$ & $\phi_1$-$\phi_6$ & $\phi_7$-$\phi_8$ \\ \hline
Eclipse & MAP & 0.37 & 0.40 & 0.37\\
Platform UI& MRR & 0.44 & 0.46 & 0.43\\ \hline
JDT& MAP & 0.35 & 0.42 & 0.36\\
& MRR & 0.43 & 0.51 & 0.45\\ \hline
SWT& MAP & 0.36 & 0.38 & 0.37\\
& MRR & 0.43 & 0.45 & 0.44\\ \hline
Birt& MAP & 0.19 & 0.21 & 0.20\\
& MRR & 0.24 & 0.27 & 0.25\\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:comparison of text similarity function}
\end{table}

Results show that the new text similarity features lead to better performance than using the original text similarity function. The new features obtain a 20\% relative improvement in terms of MAP over the \textbf{LR} approach, while features calculated based on the original text similarity function achieve only a 3\% relative improvement.

% The new text similarity function has a big impact on the system performance. The reasons of the superior of the new text similarity function over the original function on bug localization are discussed in Section~\ref{sec:text-to-code}.

\section{Evaluation of Word Embeddings for API Recommendation}
\label{sec:evaluation:SO}

% The last part of this section details a preliminary experiment using data collected from Stack Overflow for the API linking task (Section~\ref{sec:evaluation:SO}).
% An API linking task, in which queries are questions posted on Stack Overflow\footnote{http://stackoverflow.com}, and the system is supposed to identify API documents that may help the user in formulating the answer

To assess the generality of using document similarities based on word embeddings for information retrieval in software engineering, we evaluate the new similarity functions on the problem of linking API documents to Java questions posted on the community question answering (cQA) website Stack Overflow (SO). The SO website enables users to ask and answer computer programming questions, and also to vote on the quality of questions and answers posted on the website. In the Question-to-API (Q2API) linking task, the aim is to build a system that takes as input a user's question in order to identify API documents that have a non-trivial semantic overlap with the (as yet unknown) correct answer. We see such a system as being especially useful when users ask new questions, for which they would have to wait until other users post their answers. Recommending relevant API documents to the user may help the user find the answer on their own, possibly even before the correct answer is posted on the website. To the best of our knowledge, the Q2API task for cQA websites has not been addressed before.

In order to create a benchmark dataset, we first extracted all questions that were tagged with the keyword 'java', using the datadump archive available on the Stack Exchange website. Of the 1,493,883 extracted questions, we used a script to automatically select only the questions satisfying the following criteria:
\begin{enumerate}
	\item The question score is larger than 20, which means that more than 20 people have voted this question as ``useful''.
	\item The question has answers of which one was checked as the ``correct'' answer by the user who asked the question.
	\item The ``correct'' answer has a score that is larger than 10, which means that more than 10 people gave a positive vote to this answer.
	\item The ``correct'' answer contains at least one link to an API document in the official Java SE API online reference (versions 6 or 7).
\end{enumerate}
This resulted in a set of high quality 604 questions, whose correct answers contain links to Java API documents. We randomly selected 150 questions and asked two proficient Java programmers to label the corresponding API links as {\it helpful} or {\it not helpful}. The remaining 454 questions were used as a (noisy) training dataset. Out of the 150 randomly sampled questions, the 111 questions that were labeled by both annotators as having {\it helpful} API links were used for testing. The two annotators were allowed to look at the correct answer in order to determine the semantic overlap with the API document.

Although we allow API links to both versions 6 and 7, we train the word embeddings in the one-vocabulary setting, using only the Java SE 7 API documentations and tutorials. There are 5,306 documents in total, containing 3,864,850 word tokens. 

We use the Vector Space Model (VSM) as the baseline ranking system. Given a question $T$, for each API document $S$ we calculate the VSM similarity as feature $\phi_1(T, S)$ and the asymmetric semantic similarities that are based on word embeddings as features $\phi_2(T, S)$ and $\phi_3(T, S)$. In the VSM+WE system, the file score of each API document is calculated as the weighted sum of these three features, as shown in Equation~\ref{eq:file score}. During training on the 454 questions, the objective of the learning-to-rank system is to find weights such that, for each training question, the relevant (helpful) API documents are ranked at the top. During evaluation on the 111 questions in the test dataset, we rank all the Java API documents $S$ for each question $T$ in descending order of their ranking score $f(T, S)$.

\begin{table}[ht]
\centering
\caption{Results on the Q2API task.}
%\begin{adjustbox}{width=0.47\textwidth}
\begin{tabular}{|c|c|c|} \hline
Approach & MAP & MRR\\ \hline
VSM & 0.11 & 0.12\\ \hline
VSM+WE & 0.35 & 0.39 \\ \hline 
\end{tabular}
%\end{adjustbox}
\label{tab:comparison on SO questions}
\end{table}

Table~\ref{tab:comparison on SO questions} shows the MAP and MRR performance of the baseline VSM system that uses only the VSM similarity feature, vs. the performance of the VSM+WE system that also uses the two semantic similarity features. The results in this table indicate that the document similarity features based on word embeddings lead to substantial improvements in performance. As such, these results can serve as an additional empirical validation of the utility of word embeddings for information retrieval tasks in software engineering.

We note that these results are by no means the best results that we expect for this task, especially since the new features were added to a rather simple VSM baseline. For example, instead of treating SO questions only as bags of undifferentiated words, the questions could additionally be parsed in order to identify code tokens or code-like words that are then disambiguated and mapped to the corresponding API entities \cite{Bacchelli:2010:LES:1806799.1806855, Dagenais:2012:RTL:2337223.2337230, Subramanian:2014:LAD:2568225.2568313}. Given that, like VSM, these techniques are highly lexicalized, we expect their performance to improve if used in combination with additional features based on word embeddings. 
